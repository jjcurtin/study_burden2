---
title: "Make final comments" 
author: "Kendra Wyant"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---   



## Notes

This script creates a dataframe of comments to be coded.

- It filters out comments that are less than 3 words long and equate to NA (e.g., None, No, Na, No comments)

- Uses all available comments from all participants who completed burden survey (N = 247)

- Only produces first set of comments (i.e., from burden survey around month 3-4)

- results in 727 comments from 206 participants


**These final comments will be compared across coders 1 and 2 and to be used as inputs into all text analyses.**


## Set up


```{r}
#| include: false

options(conflicts.policy = "depends.ok")
library(tidyverse) 
library(janitor)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

theme_set(theme_classic())

path_burden <- format_path("risk2/data_processed/burden2")
```



## Read in data



```{r}
burden_qual <- read_csv(here::here(path_burden, "burden_qual.csv"),
                         show_col_types = FALSE) |> 
  glimpse()
```




### Format data

Removing video checkin comments because most people didnt do these and were confused by this question 



```{r}
burden_long <- burden_qual |> 
  select(-c(study_start:start_date), -video_checkin_comments) |> 
  pivot_longer(daily_updates_comments:sms_content_comments, 
               names_to = "data",
               values_to = "comment") |> 
  glimpse()
```

```{r}
burden_long <- burden_long |> 
  mutate(data = str_remove(data, "_comments"),
         data = case_when(data == "daily_updates" ~ "daily_update",
                          TRUE ~ data)) |> 
  glimpse()
```





### Filter out stop words that equate to NA or no comment

Look at comments less than 3 words to generate stop word dictionary


```{r}
stop_words <- burden_long |> 
  mutate(word_count = str_count(comment, '\\w+')) |> 
  filter(word_count < 3)

stop_words <- stop_words |> 
  filter(!comment %in% c("Accountability", "To personal", "Enjoy them", 
                         "Not preferred", "Its long", 
                         "My privqcy", "Like it",  "Privatee", "I understand", 
                         "Great survey", "Easy questions", "Easy calls", 
                         "Kinda weird", "To long", "Necessary", "Its cool", 
                         "Easy", "It helps", "Helps", "help", 
                         "Helpful", "Good experience", "Feel safe", 
                         "Reality check", "Its easy", "Nothingbyo hide"))
```



Comments to remove


```{r}
stop_words |> 
  pull(comment)

burden_long <- burden_long |> 
  filter(!is.na(comment) & !comment %in% stop_words$comment)
```



Other comments to remove - based on EDA and previous coding iterations


```{r}
# burden_long <- burden_long |> 
#   filter(!comment %in% c("Nothing at this time", "Same as previous answer", 
#                          "Same as previously", "No comments really", 
#                          "Same answers as fir the daily update, and daily videos",
#                          "Nothing to share", "Same as location", 
#                          "Same feelings for this as i am with my data", 
#                          "I have nothing to say about it", 
#                          "Feel the same way about my text messages as I feel about the phone call.",
#                          "I haven't had to say about the survey", 
#                          "Pp I love that story about it", "Pp nothing meant to say that",
#                          "Maybe this month ill do a video..", 
#                           "Oh well. Eh?", "Don't r", "I haven't had to say about the survey\n\n", 
#                          "No issues as of lately\n"))
```





### Final counts

Total comments: `r nrow(burden_long)`

Unique subids: `r length(unique(burden_long$subid))`



### Save out data


```{r}
burden_long |> 
  write_csv(here::here(path_burden, "final_comments.csv"))
```

