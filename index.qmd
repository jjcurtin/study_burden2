---
title: Understanding patient experiences with personsal sensing in a national sample of people with opioid use disorder
author:
  - name: Kendra Wyant 
    orcid: 0000-0002-0767-7589
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Abstract
date: last-modified
bibliography: references.bib
number-sections: false 
editor_options: 
  chunk_output_type: console
---

# Introduction

- In 2021, over 80,000 people in the United States died from an opioid overdose. This was the 8th leading cause of death that year, right behind diabetes. 

- Opioid use disorder is a chronic, relapsing disease, but there are very few long term supports for people in recovery.

- High risk of overdose after initial period of remission. 

- Algorithm-guided risk monitoring that uses personally sensed data and machine learning methods to predict and alert individuals when their relapse risk is high could be a potential target for providing long-term support.  

- People can comply with highly effortful sensing methods (e.g., 4 x daily EMA) while using substances [@wyantAcceptabilityPersonalSensing2023; @jonesComplianceEcologicalMomentary2019].

- Previous research has shown personal sensing to be generally acceptable to people with alcohol use disorder [@wyantAcceptabilityPersonalSensing2023]. There is reason to suspect that these findings could differ when generalizing to other substances. For example, individuals may be hesitant to provide self-report information about their use or have their location tracked if they fear legal consequences from relapsing back to use. Additionally, this sample was majority non-Hispanic White and all located in the Madison, WI, USA area.

- Like most health and mental health conditions, treatment and outcome disparities among underrepresented groups are prevalent with opioid use disorder.

- For algorithm-guided risk monitoring to be a viable support option, it is important that it is perceived to be equally acceptable across different groups of people. Without this, it is possible that providing a support tool only acceptable to a majority group, could widen existing disparities.

- It is expected that individuals will have different preferences about which sensing methods are most acceptable. However, we may see general patterns in preferences based on demographic and group characteristics. For example, working 3rd shift, being a single working mom with little time to check her phone, or living in an area with weak internet service could all present barriers to fulfilling minimum data adherence thresholds needed for self-report sensing data. Trust and related privacy concerns about providing sensitive sensed data like geolocation and text message content may be greater concern in historically marginalized groups that have experienced systemic racism and other stigma [@marwickPrivacyMarginsUnderstanding2018]. These individuals may find it more difficult to achieve privacy in their daily lives, and they may hold very different perspectives on the costs vs. benefits of surveillance in the context of personal sensing or more generally. 

- This mixed-methods study explores the acceptability and feasibility of personal sensing methods in a national sample of patients with opioid use disorder. It examines overall feasibility related to adherence and retention for providing personally sensed data for up to twelve months. It also uses two complementary qualitative methods to assess participant feedback on their experiences with various sensing methods after using the methods for 3-4 months. Finally, this study specifically assesses differences in patient adherence and feedback by sensing method and four demographic characteristics (race/ethnicity, socioeconomic status, sex at birth, and location). 

- We hope to highlight the importance of stakeholder engagement from the beginning (i.e., before an intervention is developed) and provide important considerations about the strengths, challenges, and barriers to implementing algorithm-guided risk monitoring.



# Methods

## Participants
We recruited XX<!--update with starting sample--> participants in early recovery from opioid use disorder from across the United States. We recruited through national digital advertising and collaborations with treatment providers at MOUD clinics. Our recruitment strategy was designed to create a diverse sample with respect to demographics (gender, age, race, and ethnicity), and geographic location (urban and rural). We required participants:

- were age 18 or older,
- could read, write, and speak in English,
- were enrolled in and adherent with an MOUD program for at least 1 month but no longer than 12 months or enrolled in or recently completed a day treatment program, and
- had an Android smartphone with an active data plan.  

We did not exclude participants for comorbid substance use or other psychiatric disorders. 

## Procedure
Participants completed three video or phone visits over approximately 12 months. During the enrollment visit, study staff obtained written informed consent and collected demographic information. They walked participants through how to download the Smart Technology for Addiction Recovery (STAR) study app, provided a set of video tutorials to learn how to use the app, and instructed participants to complete the intake survey within the app. The STAR app was developed by the UW Madison Center for Health Enhancement Systems Studies and used for all data collection. Within the app participants could control their data sharing options, monitor completed study tasks, receive reminder notifications about tasks, message staff, and access STAR's suite of resources and tools for people in recovery from AUD [@gustafsonSmartphoneApplicationSupport2014]. Enrolled participants met with study staff one week later to troubleshoot technical issues. At the end of the study enrollment period participants met briefly with study staff for a debriefing session. While on study, participants were expected to complete daily surveys, monthly surveys, and share cellular communicaition logs, SMS text message content and geolocation sensing data. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2019-0656). 

## Compensation
Participants were compensated \$20 for completing the onboarding phone visit, \$20 watching STAR video tutorials, \$10 for completing the troubleshooting phone visit one week later and \$10 for completing the debriefing visit at the end of the study. Participants were compensated for all study tasks completed. They were paid \$10 per month for submitting the monthly survey. They were paid up to \$15 per month for completing the daily surveys. They received \$0.25 per survey plus an additional \$3.70 for completing at least 12 surveys in the first half of the month and an additional \$3.70 for completing at least 12 surveys in the second half of the month (total range of \$14.40 - \$15.15). They also received \$5 per month for sharing geolocation data and \$10 per month for sharing cellular communication logs and text message content. Participants were also paid \$50 per month to offset the cost of maintaining a cellphone data plan. 

## Sensing Methods

### Daily Survey
Participants completed a brief (1-2 minute) 16-item daily survey each day on study through the STAR app. The daily survey became available in the app at 5:00 AM CST each morning and participants had 24 hours to complete it. Participants could enable push notifications for reminder prompts to complete the survey. Participants reported opioid lapses on the first daily survey item. If participants responded "yes" to the question "Have you used any opioids for non-medical reasons that you have not yet reported?", they were prompted to select the day(s) and time(s) of the lapse(s). Times were reported in 6-hour increments (12:00am–5:59am, 6:00am–11:59am, 12:00pm–5:59pm, 6:00pm–11:59pm). On the remaining 15 items, participants reported any other drugs that they had used, whether they took their MOUD as prescribed in the past 24 hours, and rated the maximum intensity of recent (i.e., in the past 24 hours) experiences of pain, craving, risky situations, stressful events, and pleasant events. They also rated their sleep and how depressed, angry, anxious, relaxed, and happy they have felt in the past 24 hours. Lastly, participants responded to 2 future-facing items that asked about participants’ motivation and confidence to continue to avoid using opioids for non-medical reasons over the next week. 

### Monthly Survey


### Geolocation
The STAR app passively recorded participants’ time-stamped geolocations (i.e., latitude and longitude) every 1.5-15 minutes, depending on their movement. We augmented the geolocation data with self-reported subjective context. On each monthly survey participants were asked 6 questions about frequently visited locations (i.e., locations that the participant spent more than 3 minutes on 2 or more times in a month) from the previous month. Participants were asked to describe the type of place, what they typically do there, the general frequency of pleasant and unpleasant experiences at the location, and the extent to which spending time there helps or harms their recovery. 

### Cellular Communication Logs and SMS Text Message Content
We collected cellular communication logs that included metadata about smartphone communications involving both SMS text messages and phone calls. For each communication entry, these logs include the phone number of the other party, the type of call or message (ie, incoming, outgoing, missed, or rejected), the name of the party if listed in the phone contacts, the date and time the message or call occurred, whether the log entry was read (SMS text messages only), and the duration of the call (voice calls only). For SMS text messages we also recorded the content of all incoming and outgoing messages.


## Measures

### Individual Differences
We collected self-report information about demographics (age, gender, sexual orientation, race, ethnicity, education, employment, and income). Zip codes from participants' reported home addresses were linked to Rural–Urban Commuting Area (RUCA) codes to characterize the rural–urban status of their residences [@economicresearchserviceusdepartmentofagricultureRuralUrbanCommutingArea]. These variables were collected to characterize our sample. Demographic variables with some evidence for influencing OUD treatment access and clinical outcomes and/or carrying general societal stigma (gender, race/ethnicity, income, sexual orientation, and geographic location) were used to perform subgroup analyses on acceptability ratings [@pinedoCurrentReexaminationRacial2019; @kilaruIncidenceTreatmentOpioid2020; @olfsonHealthcareCoverageService2022; @greenfieldSubstanceAbuseTreatment2007; @martinNeedReceiptSubstance2022; @lofaroRuralUrbanDifferences2025; @xinExploringIntersectionalityStigma2023]. <!--Need to decide if using all or some of these: definitely want to use race/ethnicity and rural/urban-->

We collected information about OUD history to characterize OUD severity and recovery stability across our sample. These measures included self-reported Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition OUD symptoms [@americanpsychiatricassociationDiagnosticStatisticalManual2022], past month opioid use, past month residential treatment for OUD, past month receipt of psychiatric medication, preferred opioid, preferred route of administration, and lifetime history of overdose.

### Participant Experience Questionannaire

The participant experience questionnaire was appended to the monthly survey so that participants completed it at approximately 3-4 months into the study. On the questionnaire participants responded to three items about their experience with each sensing method depending on whether the sensing method required active effort (i.e., daily survey and monthly survey) or was passively collected (i.e., geolocation, cellular communication logs, and SMS text message content). 

For the active methods, participants rated how much they agreed with two statements using 5-point bipolar scales (strongly disagree, disagree, undecided, agree, or strongly agree). The third item was an open-ended prompt. Specifically participants responded to:

1. Completing the [*sensing method*] interfered with my daily activities.

2. I disliked completing the [*sensing method*].

3. Please share any positive or negative comments you have about the [*sensing method*].

For the passive methods, we did not inquire about interference given that they required no active effort from the participant to provide the data once the app was installed and sharing permissions were turned on. For these methods participants responded to:

1. I disliked sharing my [*sensing method*].

2. Please share any positive or negative comments you have about sharing your [*sensing method*].


## Data Analysis Plan

### Quantitative Analyses
overall EMA compliance and disposition

- look at demographic differences in these two behavioral measures
- Use number of lapses as moderator? (is it harder to comply behaviorally when lapsing?)

### Qualitative Analyses
We used thematic analysis to analyze participants’ open-ended comments about the personal sensing methods. This is a systematic approach for identifying, analyzing, and reporting patterns or themes within qualitative data.  It is top-down in that it uses domain expertise from the literature to create a codebook of thematic categories driven by the aims and questions of the research being conducted. As coding is underway the codebook iteratively expands to include additional themes. We coded responses on the following themes: acceptability, sustainability, benefits, trust, usability, and feedback. We also coded valence for each theme (positive, negative, neutral). Once the code book was developed, the comments were coded by two independent coders. A third coder was brought in when discrepancies arose and these cases were discussed to arive at a resolution.
    



# Results
## Participants
A total of 336 participants enrolled in our study and provided at least one month of EMA data. [@tbl-demo] presents the demographic and clinical characteristics of these participants. 

{{< embed notebooks/mak_demo_table.qmd#tbl-demo >}} 

## Adherence

### Participation
@fig-1 shows participant attrition over 12 months. There was no significant difference in mean days on study between participants who were non-Hispanic White (*N* = 247, *M* = 299, *SD* = 109) and participants who were not White (*N* = 89, *M* = 284, *SD* = 112), *t*(334) = -1.11, *p* = .27. There was no significant difference in mean days on study between participants who reported a lapse (*N* = 1 37, *M* = 296, *SD* = 107) and those who did not report a lapse (*N* = 199, *M* = 294, *SD* = 112) while on study, *t*(334) = -0.19, *p* = .85.

{{< embed notebooks/ana_adherence.qmd#fig-1 >}}



### Monthly and Daily Updates
The overall adherence for the monthly updates was extremely high. Participants completed 96% of the monthly updates. 

The overall adherence for the daily updates was 72%. This rate is consistent with other shorter-term EMA protocols in substance use populations (Average adherence is 75% [@jonesComplianceEcologicalMomentary2019]). @fig-2 shows the overall adherence rates for the daily update by month on study. 

There was no significant difference in daily update adherence rates between participants who were non-Hispanic White (*M* = .72, *SD* = .19) and participants who were not white (*M* = .70, *SD* = .19), *t*(334) = -0.88, *p* = .38. There was a significant difference in adherence rates between participants who reported a lapse (*M* = .69, *SD* = .19) and participants who did not report a lapse (*M* = .74, *SD* = .18) while on study, *t*(334) = 2.4, *p* = .02. Participants who reported a lapse, had on average 5% lower adherence compared to participants who did not report a lapse while on study. 


{{< embed notebooks/ana_adherence.qmd#fig-2 >}}


## Participant Experience Questionannaire Quantitative Results
247 participants provided at least four months of data and completed the participant experience questionnaire. These participants were included in following analyses.


### Interference
84% of participants (*N* = 207/247) disagreed (i.e., endorsing "Strongly disagree" or "Mildly disagree") with the statement "Completing the daily update interfered with my daily activities". 79% of participants (*N* = 196/247) disagreed with the statement "Completing the monthly update interfered with my daily activities". We provide histograms of participant responses in the supplement.   

An ICC (type 3) showed that, on average, interference ratings were moderately consistent across the two updates, ICC = .50, 95% CI = [.40 - .59].


### Dislike
85% of participants (*N* = 211/247) disagreed (i.e., endorsing "Strongly disagree" or "Mildly disagree") with the statement "I disliked completing the daily update". 73% of participants (*N* = 179/246) disagreed with the statement "I disliked completing the monthly update". Fewer participants disagreed with the statement "I disliked [sensing method]" for geolocation (65%; *N* = 161/247), SMS and phone logs (64%; *N* = 158/246), and SMS content (64%; *N* = 156/245). As a result we saw an increase in responses endorsing a neutral stance toward these passive sensing methods ("Agree and disagree equally"). We provide histograms of participant responses by sensing method in the supplement.   

An ICC (type 3) showed that, on average, dislike ratings were moderately consistent across sensing methods, ICC = .48, 95% CI = [.42 - .54].




## Participant Experience Questionannaire Qualitative Results

<!-- - We began with a total of 1,356 comments. The first coder went through and identified any irrelevant comments, comments that were two words or less, while coding independently. After the excluding process, there were 647 comments in total retained and coded by both independent coders. For the results, we’ll report on the overall percentage of comments in each them and separately by data type and also differences in proportion of themes by race/ethnicity. -->

<!-- - We found a significant difference in the proportion of positive and negative comments by Hispanic participants compared to White participants for the monthly update. -->

<!-- - Positive comments were also notably less for SMS content for Black and Hispanic groups. Additionally Black participants were significantly more likely to report negative comments for about this sensing. -->

<!-- - Finally, Black participants were significantly less likely to report positive comments about geolocation compared to white participants. -->

<!-- - Looking at patterns of benefits and trust may help explain these differences.  -->

<!-- - Black participants reported virtually no benefits in their comments about geolocation and SMS Content, whereas for these same categories they reported higher percentages of comments that were related to trust for these sensing methods.  -->

<!-- - And going back to the negative comments about the monthly update in the Hispanic group. This plots on the sustainability of the methods offer additional insight. The only sensing method people in the Hispanic group made sustainability comments was about the monthly update. And these comments were much more frequent than the other groups. This suggests that perhaps the longer length of the survey or something inherent in the method was making it difficult for people in this group to complete it each month. -->



## Discussion

- On average, participants found these methods to be acceptable and saw benefits from using them.

- However, its important to acknowledge that not all participants felt this way. There were differences in acceptability of personal sensing types, specifically monthly updates from Hispanic participants and geolocation and message content from Black participants. 


- Benefits reported benefits with active methods (e.g., reflection, daily pauses, we aligning with goals). The passive methods offered no explicit benefits. We know from previous research that perceived benefits in research and healthcare play an important role in trust.

## References

::: {#refs}
:::



