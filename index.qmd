---
title: Understanding patient experiences with personsal sensing in a national sample of people with opioid use disorder
author:
  - name: Kendra Wyant 
    orcid: 0000-0002-0767-7589
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Abstract
date: last-modified
bibliography: references.bib
number-sections: false 
editor_options: 
  chunk_output_type: console
---

# Introduction

- In 2021, over 80,000 people in the United States died from an opioid overdose. This was the 8th leading cause of death that year, right behind diabetes. 

- Opioid use disorder is a chronic, relapsing disease, but there are very few long term supports for people in recovery.

- High risk of overdose after initial period of remission. 

- Algorithm-guided risk monitoring that uses personally sensed data and machine learning methods to predict and alert individuals when their relapse risk is high could be a potential target for providing long-term support.  

- People can comply with highly effortful sensing methods (e.g., 4 x daily EMA) while using substances [@wyantAcceptabilityPersonalSensing2023; @jonesComplianceEcologicalMomentary2019].

- Previous research has shown personal sensing to be generally acceptable to people with alcohol use disorder [@wyantAcceptabilityPersonalSensing2023]. There is reason to suspect that these findings could differ when generalizing to other substances. For example, individuals may be hesitant to provide self-report information about their use or have their location tracked if they fear legal consequences from relapsing back to use. Additionally, this sample was majority non-Hispanic White and all located in the Madison, WI, USA area.

- Like most health and mental health conditions, treatment and outcome disparities among underrepresented groups are prevalent with opioid use disorder.

- For algorithm-guided risk monitoring to be a viable support option, it is important that it is perceived to be equally acceptable across different groups of people. Without this, it is possible that providing a support tool only acceptable to a majority group, could widen existing disparities.

- It is expected that individuals will have different preferences about which sensing methods are most acceptable. However, we may see general patterns in preferences based on demographic and group characteristics. For example, working 3rd shift, being a single working mom with little time to check her phone, or living in an area with weak internet service could all present barriers to fulfilling minimum data adherence thresholds needed for self-report sensing data. Trust and related privacy concerns about providing sensitive sensed data like geolocation and text message content may be greater concern in historically marginalized groups that have experienced systemic racism and other stigma [@marwickPrivacyMarginsUnderstanding2018]. These individuals may find it more difficult to achieve privacy in their daily lives, and they may hold very different perspectives on the costs vs. benefits of surveillance in the context of personal sensing or more generally. 

- This mixed-methods study explores the acceptability and feasibility of personal sensing methods in a national sample of patients with opioid use disorder. It examines overall feasibility related to adherence and retention for providing personally sensed data for up to twelve months. It also uses two complementary qualitative methods to assess participant feedback on their experiences with various sensing methods after using the methods for 3-4 months. Finally, this study specifically assesses differences in patient adherence and feedback by sensing method and four demographic characteristics (race/ethnicity, socioeconomic status, sex at birth, and location). 

- We hope to highlight the importance of stakeholder engagement from the beginning (i.e., before an intervention is developed) and provide important considerations about the strengths, challenges, and barriers to implementing algorithm-guided risk monitoring.



# Methods

## Participants
We recruited 335<!--Update to enrollment number once finalized--> participants in early recovery from opioid use disorder from across the United States. We recruited through national digital advertising and collaborations with treatment providers at MOUD clinics. Our recruitment strategy was designed to create a diverse sample with respect to demographics (gender, age, race, and ethnicity), and geographic location (urban and rural). We required participants:

- were age 18 or older,
- could read, write, and speak in English,
- were enrolled in and adherent with an MOUD program for at least 1 month but no longer than 12 months or enrolled in or recently completed a day treatment program, and
- had an Android smartphone with an active data plan.  

We did not exclude participants for comorbid substance use or other psychiatric disorders. 

## Procedure
Participants completed three video or phone visits over approximately 12 months. During the enrollment visit, study staff obtained written informed consent and collected demographic information. They walked participants through how to download the Smart Technology for Addiction Recovery (STAR) study app, provided a set of video tutorials to learn how to use the app, and instructed participants to complete the intake survey within the app. The STAR app was developed by the UW Madison Center for Health Enhancement Systems Studies and used for all data collection. Within the app participants could control their data sharing options, monitor completed study tasks, receive reminder notifications about tasks, message staff, and access STAR's suite of resources and tools for people in recovery from AUD [@gustafsonSmartphoneApplicationSupport2014]. Enrolled participants met with study staff one week later to troubleshoot technical issues. At the end of the study enrollment period participants met briefly with study staff for a debriefing session. While on study, participants were expected to complete daily surveys, monthly surveys, and share cellular communicaition logs, SMS text message content and geolocation sensing data. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2019-0656). 

## Compensation
Participants were compensated \$20 for completing the onboarding phone visit, \$20 watching STAR video tutorials, \$10 for completing the troubleshooting phone visit one week later and \$10 for completing the debriefing visit at the end of the study. Participants were compensated for all study tasks completed. They were paid \$10 per month for submitting the monthly survey. They were paid up to \$15 per month for completing the daily surveys. They received \$0.25 per survey plus an additional \$3.70 for completing at least 12 surveys in the first half of the month and an additional \$3.70 for completing at least 12 surveys in the second half of the month (total range of \$14.40 - \$15.15). They also received \$5 per month for sharing geolocation data and \$10 per month for sharing cellular communication logs and text message content. Participants were also paid \$50 per month to offset the cost of maintaining a cellphone data plan. 

## Sensing Methods

### Daily Survey
Participants completed a brief (1-2 minute) 16-item survey each day on study through the STAR app. The daily survey became available in the app at 5:00 AM CST each morning and participants had 24 hours to complete it. Participants could enable push notifications for reminder prompts to complete the survey. Participants reported opioid lapses on the first daily survey item. If participants responded "yes" to the question "Have you used any opioids for non-medical reasons that you have not yet reported?", they were prompted to select the day(s) and time(s) of the lapse(s). Times were reported in 6-hour increments (12:00am–5:59am, 6:00am–11:59am, 12:00pm–5:59pm, 6:00pm–11:59pm). On the remaining 15 items, participants reported any other drugs that they had used, whether they took their MOUD as prescribed in the past 24 hours, and rated the maximum intensity of recent (i.e., in the past 24 hours) experiences of pain, craving, risky situations, stressful events, and pleasant events. They also rated their sleep and how depressed, angry, anxious, relaxed, and happy they have felt in the past 24 hours. Lastly, participants responded to 2 future-facing items that asked about participants’ motivation and confidence to continue to avoid using opioids for non-medical reasons over the next week. 

### Monthly Survey
Particpants completed one longer (20-30 minute) survey each month through the STAR app. The first monthly survey became available immediately after participants enrolled in the study. The first survey measured individual trait and clinical characteristics. Specifically, it measured demographics, lifetime substance use history [@whoassistworkinggroupAlcoholSmokingSubstance2002], opioid treatment history, Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition, OUD diagnostic criteria [@americanpsychiatricassociationDiagnosticStatisticalManual2022], distress tolerance [@simonsDistressToleranceScale2005], pain catastrophizing [@crombezChildVersionPain2003; @sullivanPainCatastrophizingScale1995], personality traits relevant to psychopathology [@gomezPersonalityInventoryDSM2020; @kruegerInitialConstructionMaladaptive2012], adverse childhood experiences [@murphyAdverseChildhoodExperiences2014], and trauma experience [@frissaChallengesRetrospectiveAssessment2016]. 

All monthly surveys (first and later) measured dynamic risk factors, including changes in life circumstances (e.g., employment status and living situation), social connectedness [@sherbourneMOSSocialSupport1991], romantic relationship quality [@hendrickGenericMeasureRelationship1988], psychiatric symptoms [@eisenBASISDevelopmentSelfreport1986], pain [@dautDevelopmentWisconsinBrief1983; @krebsDevelopmentInitialValidation2009], stress [@cohenGlobalMeasurePerceived1983], quality of life [@skevingtonWorldHealthOrganizations2004], substance use [@whoassistworkinggroupAlcoholSmokingSubstance2002], recovery satisfaction, motivation, and confidence, and questions about treatment use, adherence, and perceived efficacy [@moriskyPredictiveValidityMedication2008; @voilsInitialValidationSelfreport2012].

Lastly, at the end of each later survey (i.e., excluding the first) we asked additional questions about the intrapersonal and subjective context associated with people and places with which the participant has frequent contact or visits in the previous month (see Geolocation and Cellular Communication Logs and SMS Text Message Content). 

### Geolocation
The STAR app passively recorded participants’ time-stamped geolocations (i.e., latitude and longitude) every 1.5-15 minutes, depending on their movement. We augmented the geolocation data with self-reported subjective context. On each monthly survey participants were asked 6 questions about frequently visited locations (i.e., locations that the participant spent more than 3 minutes on 2 or more times in a month) from the previous month. Participants were asked to describe the type of place, what they typically do there, the general frequency of pleasant and unpleasant experiences at the location, and the extent to which spending time there helps or harms their recovery. 

### Cellular Communication Logs and SMS Text Message Content
We collected cellular communication logs that included metadata about smartphone communications involving both SMS text messages and phone calls. For each communication entry, these logs include the phone number of the other party, the type of call or message (ie, incoming, outgoing, missed, or rejected), the name of the party if listed in the phone contacts, the date and time the message or call occurred, whether the log entry was read (SMS text messages only), and the duration of the call (voice calls only). For SMS text messages we also recorded the content of all incoming and outgoing messages. On each monthly survey participants were asked 5 questions about contacts with whom participants frequently communicated in the previous month (i.e., at least 2 calls 2 minutes or longer in duration, 1 call 15 minutes or longer in duration, or 4 SMS text messages). Participants were asked to describe the nature of their relationship with the contact, the general frequency of pleasant and unpleasant interactions associated with the person, and the extent to which interactions with the contact support or undermine their recovery.

## Measures 

### Individual Differences
We collected self-report information about demographics (age, gender, sexual orientation, race, ethnicity, education, employment, and income). Zip codes from participants' reported home addresses were linked to Rural–Urban Commuting Area (RUCA) codes to characterize the rural–urban status of their residences [@economicresearchserviceusdepartmentofagricultureRuralUrbanCommutingArea]. These variables were collected to characterize our sample. Demographic variables with demonstrated discrepancies in OUD treatment access and clinical outcomes (gender, race/ethnicity and geographic location) were used to perform subgroup analyses on acceptability measures [@pinedoCurrentReexaminationRacial2019; @kilaruIncidenceTreatmentOpioid2020; @greenfieldSubstanceAbuseTreatment2007; @martinNeedReceiptSubstance2022; @lofaroRuralUrbanDifferences2025].

We collected information about OUD history to characterize OUD severity and recovery stability across our sample. These measures included self-reported Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition OUD symptoms [@americanpsychiatricassociationDiagnosticStatisticalManual2022], past month opioid use, past month residential treatment for OUD, past month receipt of psychiatric medication, preferred opioid, preferred route of administration, and lifetime history of overdose.

### Participant Experience Questionannaire

The participant experience questionnaire was appended to the monthly survey so that participants completed it at approximately 3-4 months into the study. On the questionnaire participants responded to three items about their experience with each sensing method depending on whether the sensing method required active effort (i.e., daily survey and monthly survey) or was passively collected (i.e., geolocation, cellular communication logs, and SMS text message content). 

For the active methods, participants rated how much they agreed with two statements using 5-point bipolar scales (strongly agree, agree, undecided, disagree, or strongly disagree). The third item was an open-ended prompt. Specifically participants responded to:

1. Completing the [*sensing method*] interfered with my daily activities.

2. I disliked completing the [*sensing method*].

3. Please share any positive or negative comments you have about the [*sensing method*].

For the passive methods, we did not inquire about interference given that they required no active effort from the participant to provide the data once the app was installed and sharing permissions were turned on. For these methods participants responded to:

1. I disliked sharing my [*sensing method*].

2. Please share any positive or negative comments you have about sharing your [*sensing method*].


## Data Analysis Plan

### Participation and Adherence
For all enrolled participants we calculated two behavioral indicators of acceptability with the study protocol. First, we reported the overall average length of participation in days. We also plot monthly retention rates over the course of the 12-month study. Second, we reported overall and monthly adherence rates for the 2 active personal sensing methods (daily and monthly surveys). We did not calculate adherence for our three passive sensing methods (geolocation, cellular communication logs and SMS text message content) because it was not possible to determine whether missing data was due to non-adherence (e.g., disabling sharing permissions) or other reasons (e.g., infrequent communications with other people, phone in battery-saving mode, poor cellular connection).

We performed 3 dichotomous demographic subgroup comparisons to determine if certain subgroups were better able and more willing to adhere to the study protocol. Specifically, we regressed length of participation on each of the key demographic grouping variables (gender, race/ethnicity and geographic location).<!--change to t test language?--> We did not assess for subgroup differences in daily and monthly survey adherence rates to minimize the number of comparisons. Participants were withdrawn from the study if adherence dropped, thus differences in adherence will likely also be captured in length of participation. 

### Participant Experience Questionannaire Quantitative Analyses
Participants rated how much they agreed with statements about sensing method interference and dislike using a 5-point scale with descriptive anchors (strongly agree, agree, undecided, disagree, or strongly disagree). We recoded these items to a numeric scale ranging from −2 to 2, with 0 representing the neutral (undecided) midpoint and higher scores representing greater acceptability. We assessed overall participant perception of the sensing methods in two ways. First, we conducted 2-tailed, 1 sample *t* tests for the 2 items for each personal sensing method to determine whether on average perceptions of the sensing method are different from neutral (i.e., 0/undecided). We also calculated intraclass correlations (single, case 3 [@shroutIntraclassCorrelationsUses1979]) separately for the interference and dislike item to quantify the consistency of perceptions across personal sensing methods (e.g., Do participants who dislike one method also dislike the other methods?).

<!--discuss how to approach demographic subgroup comparisons:

- 7 overall t-tests (2 for interference, 5 for dislike)
- 3 subgroups

-->

### Participant Experience Questionannaire Qualitative Analyses
We used thematic analysis to analyze participants’ open-ended comments about the personal sensing methods. This is a systematic approach for identifying, analyzing, and reporting patterns or themes within qualitative data.  It is top-down in that it uses domain expertise from the literature to create a codebook of thematic categories driven by the aims and questions of the research being conducted. As coding is underway, the codebook iteratively expands to include additional themes. Once the code book was developed, the comments were coded by two independent coders. A third coder was brought in when discrepancies arose and these cases were discussed to arive at a resolution. The final codebook consisted of 6 themes: acceptability, sustainability, benefits, trust, usability, and feedback. For each endorsed theme, we also coded valence (positive, negative, neutral). <!--add table of theme definitions here or in supplement-->

We calculated proportions of themes endorsed overall and by subgroup. <!--add comparisons among subgroups--> <!--dichotomizing will lose some nuance we saw in Olivia's poster - report these in supplement-->
    

# Results

### Participation and Adherence
A total of 335 participants enrolled in our study<!--update to enrollment number-->. [@tbl-demo] presents the demographic and clinical characteristics of these participants. 

{{< embed notebooks/mak_demo_table.qmd#tbl-demo >}} 

<!--update results to enrollment sample-->
@fig-1 shows participant retention over 12 months. <!--report 5 subgroup comparisons-->

{{< embed notebooks/ana_adherence.qmd#fig-1 >}}


The overall adherence for the monthly updates was extremely high. Participants completed 96% of the monthly updates. The overall adherence for the daily updates was 72%. @fig-2 shows the overall adherence rates for the daily update by month on study. <!--update-->


{{< embed notebooks/ana_adherence.qmd#fig-2 >}}


## Participant Experience Questionannaire Quantitative Results
247 participants completed the participant experience questionnaire. These participants were included in following analyses. <!--sensitivity analyses of demographics in analysis vs. enrolled sample-->


### Interference
<!--Overall acceptability by sensing method-->

<!-- An ICC (type 3) showed that, on average, interference ratings were moderately consistent across the two updates, ICC = .50, 95% CI = [.40 - .59]. -->

<!--We provide histograms of participant responses by sensing method in the supplement.-->

<!--figure of mean responses with confidence intervals for each subgroup, pairwise comparisons?-->

### Dislike
<!--Overall acceptability by sensing method-->

<!-- An ICC (type 3) showed that, on average, dislike ratings were moderately consistent across sensing methods, ICC = .48, 95% CI = [.42 - .54]. -->


<!--We provide histograms of participant responses by sensing method in the supplement.-->

<!--figure of mean responses with confidence intervals for each subgroup, pairwise comparisons?-->


## Participant Experience Questionannaire Qualitative Results

<!-- - We began with a total of 1,356 comments. The first coder went through and identified any irrelevant comments, comments that were two words or less, while coding independently. After the excluding process, there were 647 comments in total retained and coded by both independent coders. For the results, we’ll report on the overall percentage of comments in each them and separately by data type and also differences in proportion of themes by race/ethnicity. -->

<!-- - We found a significant difference in the proportion of positive and negative comments by Hispanic participants compared to White participants for the monthly update. -->

<!-- - Positive comments were also notably less for SMS content for Black and Hispanic groups. Additionally Black participants were significantly more likely to report negative comments for about this sensing. -->

<!-- - Finally, Black participants were significantly less likely to report positive comments about geolocation compared to white participants. -->

<!-- - Looking at patterns of benefits and trust may help explain these differences.  -->

<!-- - Black participants reported virtually no benefits in their comments about geolocation and SMS Content, whereas for these same categories they reported higher percentages of comments that were related to trust for these sensing methods.  -->

<!-- - And going back to the negative comments about the monthly update in the Hispanic group. This plots on the sustainability of the methods offer additional insight. The only sensing method people in the Hispanic group made sustainability comments was about the monthly update. And these comments were much more frequent than the other groups. This suggests that perhaps the longer length of the survey or something inherent in the method was making it difficult for people in this group to complete it each month. -->



## Discussion

- On average, participants found these methods to be acceptable and saw benefits from using them.

- The overall adherence for the daily updates was 72%. This rate is consistent with other shorter-term EMA protocols in substance use populations (Average adherence is 75% [@jonesComplianceEcologicalMomentary2019]). Suggests feasibility of long-term EMA

- Saw an increase in responses endorsing a neutral stance toward these passive sensing methods ("Agree and disagree equally"). 

- However, its important to acknowledge that not all participants felt this way. There were differences in acceptability of personal sensing types, specifically monthly updates from Hispanic participants and geolocation and message content from Black participants. 


- Benefits reported benefits with active methods (e.g., reflection, daily pauses, aligning with goals). The passive methods offered no explicit benefits. We know from previous research that perceived benefits in research and healthcare play an important role in trust.

## References

::: {#refs}
:::



