---
title: "Topic Modeling Analyses" 
author: "Kendra and Olivia"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true 
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---   

## Notes

Topic Modeling Analyses to determine data-driven topics overall, by data type and demographic.

https://www.tidytextmining.com/dtm
https://www.tidytextmining.com/topicmodeling

- Notes to improve topic modeling
- lemitization
- topics by document? Different documents (dem, data)?




## Set up
```{r}
#| include: false

options(conflicts.policy = "depends.ok")
library(tidyverse) 
library(tidytext)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

theme_set(theme_classic())

path_prep <- format_path("studydata/risk2/data_processed/prep")
```

## Read in data

```{r}
comments <- read_csv(file.path(path_prep, "final_coded_comments.csv"),
                   show_col_types = FALSE) 

dem <- read_csv(here::here(path_prep, "demographics.csv"),
                show_col_types = FALSE)

comments <- comments |> 
  left_join(dem, by = "subid") |> 
  mutate(race = factor(race, levels = c("White/Caucasian", "Black/African American",
                                        "Hispanic, Latino, or Spanish origin", "other"))) |> # sets white as reference var
  mutate(id = row_number())  # sets each comment to its own document id
```


## Tokenize Comments
```{r}

custom_stop_words <- tibble(word = c("daily", "update", "updates", "gps", "monthly", "logs",
                                     "dont", "mind", "video", "im", "doesnt", "bother", "data"),
                            lexicon = c("CUSTOM"))

# Bind the custom stop words to stop_words
stop_words <- stop_words %>% 
  bind_rows(custom_stop_words)

tidy_comments <- comments |> 
  unnest_tokens(word, answer) |> # tokenize comments
  anti_join(stop_words) |> 
  mutate(stem = SnowballC::wordStem(word))

tidy_comments |> 
  count(stem) |> 
  arrange(desc(n)) |> 
  print(n = 20)
```



## STM

```{r}
library(stm)

tidy_sparse <- tidy_comments |> 
  count(id, stem) |> 
  cast_sparse(id, stem, n)
```


```{r}
covariate_df <- tidy_comments |> 
  group_by(id) |> 
  slice(1) |> 
  filter(!is.na(race)) |> 
  select(id, race, data, stem) 

tidy_sparse_covariate <- tidy_comments |> 
  filter(!is.na(race)) |>
  count(id, stem) |> 
  cast_sparse(id, stem, n)

covariate_topic_model <- stm(tidy_sparse_covariate, K = 15, 
                             prevalence = ~ race + data,
                             data = covariate_df, 
                             verbose = FALSE)

summary(covariate_topic_model)


td_beta <- tidy(covariate_topic_model)
td_beta <- tidy_comments |>
  select(term = stem, word) |>
  group_by(term) |>
  slice(1) |>
  right_join(td_beta, by = "term")

td_beta %>%
    group_by(topic) %>%
  filter(topic == 5) |> 
    slice_max(beta, n = 20) %>%
    ungroup() %>%
    ggplot(aes(beta, word)) +
    geom_col() +
    facet_wrap(~ topic, scales = "free")
```

```{r}
labelTopics(covariate_topic_model, n = 15)


plot(covariate_topic_model, n = 3)


```


```{r}
topic_effects <- estimateEffect(~ race, covariate_topic_model, metadata = covariate_df)


summary(topic_effects)

plot(topic_effects, covariate = "race", topics = c(5, 9, 11, 13, 14), model = covariate_topic_model, method = "difference",cov.value1 = "White/Caucasian", cov.value2 = "Black/African American")


tidy(topic_effects) |> 
  filter(topic %in% c(5, 9, 11, 13, 14) & term == "raceBlack/African American") |> 
  mutate(topic = factor(topic, levels = c(5, 9, 11, 13, 14))) |> 
  ggplot(aes(x = estimate, y = topic, group = topic)) +
  geom_point() +
  geom_linerange(aes(xmin = estimate - std.error, xmax = estimate + std.error)) +
  xlim(c(-.15, .15)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Difference in proportion of topics among Black compared to White participants")

tidy(topic_effects) |> 
  filter(topic %in% c(11, 2) & term == "raceHispanic, Latino, or Spanish origin") |> 
  mutate(topic = factor(topic, levels = c( 2, 11))) |> 
  ggplot(aes(x = estimate, y = topic, group = topic)) +
  geom_point() +
  geom_linerange(aes(xmin = estimate - std.error, xmax = estimate + std.error)) +
  xlim(c(-.15, .2)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Difference in proportion of topics among Hispanic compared to White participants")
```






## latent Dirichlet allocation (LDA)

Create document term matric (DTM)
```{r}
tidy_dtm <- tidy_comments |> 
  count(stem, subid) |> 
  cast_dtm(subid, stem, n) |> 
  as.matrix()
```

Run topic model
```{r}
library(topicmodels)

lda <- LDA(tidy_dtm, 
           k = 3, # number of topics
           method = "Gibbs",
           control = list(seed = 42))
```

```{r}
lda_topics <- lda |> 
  tidy(matrix = "beta") |> 
  arrange(desc(beta))

word_probs <- lda_topics |> 
  group_by(topic) |> 
  slice_max(beta, n = 15) |> 
  ungroup() |> 
  mutate(term = fct_reorder(term, beta))
```

```{r}
word_probs |> 
  ggplot(aes(x = term,
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()
```


### Geolocation

```{r}
tidy_comments_gps <- comments |> 
  filter(data == "gps") |> 
  mutate(id = row_number()) |> # sets each comment to its own document id
  unnest_tokens(word, answer) |> # tokenize comments
  anti_join(stop_words) |> 
  mutate(stem = SnowballC::wordStem(word))

tidy_comments_gps |> 
  count(word) |> 
  arrange(desc(n)) |> 
  print(n = 20)
```

```{r}
tidy_dtm_gps <- tidy_comments_gps |> 
  count(word, subid) |> 
  cast_dtm(subid, word, n) |> 
  as.matrix()
```

```{r}
lda_gps <- LDA(tidy_dtm_gps, 
           k = 3, # number of topics
           method = "Gibbs",
           control = list(seed = 42))
```

```{r}
lda_topics_gps <- lda_gps |> 
  tidy(matrix = "beta") |> 
  arrange(desc(beta))

word_probs_gps <- lda_topics_gps |> 
  group_by(topic) |> 
  slice_max(beta, n = 10) |> 
  ungroup() |> 
  mutate(term = fct_reorder(term, beta))
```

```{r}
word_probs_gps |> 
  ggplot(aes(x = term,
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()
```

### Race/ethnicity and all comments

```{r}
dem <- read_csv(here::here(path_prep, "demographics.csv"),
                show_col_types = FALSE)

comments_dem <- comments |> 
  left_join(dem, by = "subid")
```

Black
```{r}
tidy_comments_race <- comments_dem |> 
  filter(race == "Black/African American") |> 
  mutate(id = row_number()) |> # sets each comment to its own document id
  unnest_tokens(word, answer) |> # tokenize comments
  anti_join(stop_words) |> 
  mutate(stem = SnowballC::wordStem(word))

tidy_comments_race |> 
  count(stem) |> 
  arrange(desc(n)) |> 
  print(n = 20)
```

```{r}
tidy_dtm_race <- tidy_comments_race |> 
  count(stem, subid) |> 
  cast_dtm(subid, stem, n) |> 
  as.matrix()
```

```{r}
lda_race <- LDA(tidy_dtm_race, 
           k = 3, # number of topics
           method = "Gibbs",
           control = list(seed = 42))
```

```{r}
lda_topics_race <- lda_race |> 
  tidy(matrix = "beta") |> 
  arrange(desc(beta))

word_probs_race <- lda_topics_race |> 
  group_by(topic) |> 
  slice_max(beta, n = 10) |> 
  ungroup() |> 
  mutate(term = fct_reorder(term, beta))
```

```{r}
word_probs_race |> 
  ggplot(aes(x = term,
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()
```

White
```{r}
tidy_comments_race <- comments_dem |> 
  filter(race == "White/Caucasian") |> 
  mutate(id = row_number()) |> # sets each comment to its own document id
  unnest_tokens(word, answer) |> # tokenize comments
  anti_join(stop_words)

tidy_comments_race |> 
  count(word) |> 
  arrange(desc(n)) |> 
  print(n = 20)
```

```{r}
tidy_dtm_race <- tidy_comments_race |> 
  count(word, subid) |> 
  cast_dtm(subid, word, n) |> 
  as.matrix()
```

```{r}
lda_race <- LDA(tidy_dtm_race, 
           k = 3, # number of topics
           method = "Gibbs",
           control = list(seed = 42))
```

```{r}
lda_topics_race <- lda_race |> 
  tidy(matrix = "beta") |> 
  arrange(desc(beta))

word_probs_race <- lda_topics_race |> 
  group_by(topic) |> 
  slice_max(beta, n = 10) |> 
  ungroup() |> 
  mutate(term = fct_reorder(term, beta))
```

```{r}
word_probs_race |> 
  ggplot(aes(x = term,
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()
```

Hispanic 
```{r}
tidy_comments_race <- comments_dem |> 
  filter(race == "Hispanic, Latino, or Spanish origin") |> 
  mutate(id = row_number()) |> # sets each comment to its own document id
  unnest_tokens(word, answer) |> # tokenize comments
  anti_join(stop_words)

tidy_comments_race |> 
  count(word) |> 
  arrange(desc(n)) |> 
  print(n = 20)
```

```{r}
tidy_dtm_race <- tidy_comments_race |> 
  count(word, subid) |> 
  cast_dtm(subid, word, n) |> 
  as.matrix()
```

```{r}
lda_race <- LDA(tidy_dtm_race, 
           k = 3, # number of topics
           method = "Gibbs",
           control = list(seed = 42))
```

```{r}
lda_topics_race <- lda_race |> 
  tidy(matrix = "beta") |> 
  arrange(desc(beta))

word_probs_race <- lda_topics_race |> 
  group_by(topic) |> 
  slice_max(beta, n = 10) |> 
  ungroup() |> 
  mutate(term = fct_reorder(term, beta))
```

```{r}
word_probs_race |> 
  ggplot(aes(x = term,
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()
```

### Race/ethnicity and gps

```{r}
dem <- read_csv(here::here(path_prep, "demographics.csv"),
                show_col_types = FALSE)

comments_dem <- comments |> 
  left_join(dem, by = "subid")
```

Black
```{r}
tidy_comments_race <- comments_dem |> 
  filter(data == "gps" & race == "Black/African American") |> 
  mutate(id = row_number()) |> # sets each comment to its own document id
  unnest_tokens(word, answer) |> # tokenize comments
  anti_join(stop_words)

tidy_comments_race |> 
  count(word) |> 
  arrange(desc(n)) |> 
  print(n = 20)
```

```{r}
tidy_dtm_race <- tidy_comments_race |> 
  count(word, subid) |> 
  cast_dtm(subid, word, n) |> 
  as.matrix()
```

```{r}
lda_race <- LDA(tidy_dtm_race, 
           k = 2, # number of topics
           method = "Gibbs",
           control = list(seed = 42))
```

```{r}
lda_topics_race <- lda_race |> 
  tidy(matrix = "beta") |> 
  arrange(desc(beta))

word_probs_race <- lda_topics_race |> 
  group_by(topic) |> 
  slice_max(beta, n = 10) |> 
  ungroup() |> 
  mutate(term = fct_reorder(term, beta))
```

```{r}
word_probs_race |> 
  ggplot(aes(x = term,
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()
```

White
```{r}
tidy_comments_race <- comments_dem |> 
  filter(data == "gps" & race == "White/Caucasian") |> 
  mutate(id = row_number()) |> # sets each comment to its own document id
  unnest_tokens(word, answer) |> # tokenize comments
  anti_join(stop_words)

tidy_comments_race |> 
  count(word) |> 
  arrange(desc(n)) |> 
  print(n = 20)
```

```{r}
tidy_dtm_race <- tidy_comments_race |> 
  count(word, subid) |> 
  cast_dtm(subid, word, n) |> 
  as.matrix()
```

```{r}
lda_race <- LDA(tidy_dtm_race, 
           k = 2, # number of topics
           method = "Gibbs",
           control = list(seed = 42))
```

```{r}
lda_topics_race <- lda_race |> 
  tidy(matrix = "beta") |> 
  arrange(desc(beta))

word_probs_race <- lda_topics_race |> 
  group_by(topic) |> 
  slice_max(beta, n = 10) |> 
  ungroup() |> 
  mutate(term = fct_reorder(term, beta))
```

```{r}
word_probs_race |> 
  ggplot(aes(x = term,
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()
```

Hispanic - not enough data to make more than one category
